{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Mini-Project - Find signature\n",
    "\n",
    "Katie explained in a video a problem that arose in preparing Chris and Sara’s email for the author identification project; it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair advantage to an algorithm). You’ll work through that discovery process here.\n",
    "\n",
    "\n",
    "##  Overfitting a Decision Tree 1\n",
    "\n",
    "This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.  \n",
    "\n",
    "#### If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?\n",
    "Ans : Pretty low\n",
    "\n",
    "#### If a decision tree is overfit, would you expect high or low accuracy on the training set?\n",
    "Ans : High  \n",
    "\n",
    "The accuracy would be very high on the training set, but would plummet once it was actually tested.\n",
    "\n",
    "## Number of Features and Overfitting\n",
    "\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. \n",
    "\n",
    "You can find the starter code in ```feature_selection/find_signature.py```.   \n",
    "Get a decision tree up and training on the training data, and print out the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starter code\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starter code\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../text_learning/your_word_data.pkl\" \n",
    "authors_file = \"../text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\py2_env\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Starter code\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import cross_validation\n",
    "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, \n",
    "                                                                                             random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "\n",
    "# the vectorizer is being fitted on the training features. This will allow it to build its list of \n",
    "# vocabulary to generate features and also get feature names.\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starter code\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many training points are there, according to the starter code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup! We've limited our training data quite a bit, so we should be expecting our models to potentially overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Your Overfit Decision Tree\n",
    "\n",
    "What’s the accuracy of the decision tree you just made? (Remember, we're setting up our decision tree to overfit -- ideally, we want to see the test accuracy as relatively low.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT classifier is :  0.816837315131\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, pred)\n",
    "print \"Accuracy of DT classifier is : \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the test performance has an accuracy much higher than it is expected to be - if we are overfitting, then the test performance should be relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the Most Powerful Features\n",
    "\n",
    "Take your (overfit) decision tree and use the ```feature_importances_``` attribute to get a list of the relative importance of all the features being used. \n",
    "\n",
    "We suggest iterating through this list (it’s long, since this is text data) and only printing out the feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give an importance of far less than 0.01). What’s the importance of the most important feature? What is the number of this feature?\n",
    "\n",
    "__Guidance:__  The object of this exercise is to analyze which features are most predictive or most important. The output should be all features above a certain threshold and their feature importance and feature number. The feature importance of the most important feature and the number of that feature are what should be entered into the quiz.  \n",
    "\n",
    "One way to proceed would be to run the code and look at the output (the printed output). You may consider playing with the threshold and seeing how this impacts the output.\n",
    "\n",
    "The number we get from \"identifying the most powerful features\" is actually the index of for that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Important feature :  0.363636363636\n",
      "Feature Number :  21323\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(importances)):\n",
    "    if importances[i] > 0.2:\n",
    "        print \"Most Important feature : \",importances[i]\n",
    "        print \"Feature Number : \",i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.105378579003\n",
      "0.186927243449\n",
      "0.363636363636\n"
     ]
    }
   ],
   "source": [
    "for i in importances:\n",
    "    if i > 0.1:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- __Deciding on a threshold :__ This will vary from model to model. The feature importance is somewhat of a measure of how much information we gain from using that feature as measured by the impact the split has on overall system purity. That is feature splits that decrease the impurity of the system more are more important. Often we look at several features and choose a threshold based on using a reasonable number of features with reasonably high scores relative to other features. For example in the plot below we would likely choose the first three features, after which there is a drop off in importance.\n",
    "\n",
    "![feature_imp](images/feature_imp.png)\n",
    "\n",
    "- To get the most important feature we don't necessarily need to set a threshold as we do in the code above. You could also return all feature importances and sort. Setting a threshold is a good idea because it filters out features we wouldn't consider important and gives us a smaller list to work with.\n",
    "\n",
    "- To get the feature number we can use the index of importances. If we determine the index of the highest scoring feature, this can be used to determine the feature number. There are other approaches as well, such as counting through the feature iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use TfIdf to Get the Most Important Word\n",
    "\n",
    "In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that you obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in the TfIdf by calling ```get_feature_names()``` on it; pull out the word that’s causing most of the discrimination of the decision tree. What is it? Does it make sense as a word that’s uniquely tied to either __Chris Germany__ or __Sara Shackleton__, a signature of sorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'houectect'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_list = vectorizer.get_feature_names()\n",
    "words_list[21323]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most powerful word when the decision tree is making its classification decision.\n",
    "\n",
    "Even though our training data is limited, we still have a word that is highly indicative of author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove, Repeat\n",
    "\n",
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to ```text_learning/vectorize_text.py```, and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun ```vectorize_text.py```, and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? (Define an outlier as a feature with importance >0.2, as before).\n",
    "\n",
    "After removing the first signature word, another powerful signature word arises.\n",
    "\n",
    "__cgermannsf__\n",
    "\n",
    "## Checking Important Features Again\n",
    "\n",
    "Update ```vectorize_test.py``` one more time, and rerun. Then run ```find_signature.py``` again. Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?\n",
    "\n",
    "__houectect__  \n",
    "\n",
    "Yes, there is one more word (\"houectect\").  Your guess about what this word means is as good as ours, but it doesn't look like an obvious signature word so let's keep moving without removing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the Overfit Tree\n",
    "\n",
    "What’s the accuracy of the decision tree now? We've removed two \"signature words\", so it will be more difficult for the algorithm to fit to our limited training set without overfitting. Remember, the whole point was to see if we could get the algorithm to overfit--a sensible result is one where the accuracy isn't that great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DT classifier is :  0.816837315131\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(labels_test, pred)\n",
    "print \"Accuracy of DT classifier is : \", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've removed the outlier \"signature words\", the training data is starting to overfit to the words that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
